{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3493db47",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "pd.set_option('display.max_rows', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cf1f4f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "identity = pd.read_csv('data/train_identity.csv')\n",
    "transaction = pd.read_csv('data/train_transaction.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d434727b",
   "metadata": {},
   "outputs": [],
   "source": [
    "identity.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01f9c3c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "transaction.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f56893dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in transaction.columns:\n",
    "    if transaction[col].dtype == 'object':\n",
    "        print(f\"{col}: {transaction[col].nunique()} unique values\")\n",
    "    else:\n",
    "        print(f\"{col}: {transaction[col].nunique()} unique values\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c674668",
   "metadata": {},
   "source": [
    "# Merge train and test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "647822df",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = transaction.merge(identity, on=\"TransactionID\", how=\"left\")\n",
    "\n",
    "# Save merged train\n",
    "train.to_csv(\"data/train_merged.csv\", index=False)\n",
    "\n",
    "# Load and merge test\n",
    "test_identity = pd.read_csv(\"data/test_identity.csv\")\n",
    "test_transaction = pd.read_csv(\"data/test_transaction.csv\")\n",
    "test = test_transaction.merge(test_identity, on=\"TransactionID\", how=\"left\")\n",
    "\n",
    "# Save merged test\n",
    "test.to_csv(\"data/test_merged.csv\", index=False)\n",
    "\n",
    "print(\"Train merged shape:\", train.shape)\n",
    "print(\"Test merged shape:\", test.shape)\n",
    "print(\"Files saved as data/train_merged.csv and data/test_merged.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "699ecb66",
   "metadata": {},
   "source": [
    "# EDA "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffea6b9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "pd.set_option('display.max_rows', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7740188b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"data/train_merged.csv\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b4cad77",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info() "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36b6a4e5",
   "metadata": {},
   "source": [
    "# Overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6527f4a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1455898d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in df.columns:\n",
    "    if df[col].dtype == 'object':\n",
    "        print(f\"#{col}: {df[col].nunique()} unique values\")\n",
    "        print(f'sample values: {df[col].dropna().unique()[:5]}')\n",
    "    else:\n",
    "        print(f\"#{col}: {df[col].nunique()} unique values\")\n",
    "        print(f'sample values: {df[col].dropna().unique()[:5]}') "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8ce8623",
   "metadata": {},
   "source": [
    "# ðŸ“Š Feature Explanation â€“ IEEE-CIS Fraud Detection\n",
    "\n",
    "## 1. Transaction Features\n",
    "- **TransactionID**: Unique identifier for each transaction. Used to merge with the identity table, not directly useful for modeling.  \n",
    "- **TransactionDT**: Time in seconds from a reference point (not a real timestamp).\n",
    "- **TransactionAmt**: Transaction amount. Fraud often involves:  \n",
    "  - Very small amounts (testing stolen cards).  \n",
    "  - Very large amounts (cash-out quickly).  \n",
    "- **ProductCD**: Product category (e.g., W, C, H, R, S).  \n",
    "\n",
    "---\n",
    "\n",
    "## 2. Card Features (card1â€“card6)\n",
    "- **card1**: User/account identifier (anonymized).\n",
    "- **card2**: Issuing bank (anonymized).  \n",
    "- **card3**: Country of issuing bank.  \n",
    "- **card4**: Card type (Visa, MasterCard, Amex, etc.).  \n",
    "- **card5**: Card series number.  \n",
    "- **card6**: Card category (credit/debit).    \n",
    "\n",
    "---\n",
    "\n",
    "## 3. Address Features\n",
    "- **addr1**: Billing region.  \n",
    "- **addr2**: Billing country.  \n",
    "\n",
    "---\n",
    "\n",
    "## 4. Email Features\n",
    "- **P_emaildomain**: Purchaserâ€™s email domain.  \n",
    "- **R_emaildomain**: Recipientâ€™s email domain.  \n",
    "\n",
    "---\n",
    "\n",
    "## 5. Counting Features (C1â€“C14)\n",
    "- Pre-engineered **count statistics** (anonymized).  \n",
    "- Example meaning: number of transactions linked to a card or user.  \n",
    "\n",
    "---\n",
    "\n",
    "## 6. Time Delta Features (D1â€“D15)\n",
    "- Pre-engineered **time-related deltas**.  \n",
    "- Examples:  \n",
    "  - `D1` â‰ˆ days since first transaction of user.  \n",
    "  - `D10` â‰ˆ days since last billing.  \n",
    "\n",
    "---\n",
    "\n",
    "## 7. Matching Features (M1â€“M9)\n",
    "- Boolean flags (Yes/No/NaN).  \n",
    "- Indicate whether information matches across sources (e.g., billing vs. shipping address, email vs. card info).  \n",
    "\n",
    "---\n",
    "\n",
    "## 8. Engineered Features (V1â€“V339)\n",
    "- Large set of anonymized engineered features.  \n",
    "- Likely derived from C/D/M variables via normalization, statistical transformations, or PCA.  \n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "## 9. Identity Features\n",
    "- **DeviceType**: Desktop or mobile.  \n",
    "- **DeviceInfo**: OS/browser/device information.  \n",
    "- **id_01â€“id_38**: Digital identity flags (proxy usage, cookies, authentication methods, risk scores).  \n",
    "\n",
    "---\n",
    "\n",
    "## âœ… Summary\n",
    "- **Transaction-related features**: core info (time, amount, product).  \n",
    "- **Card/Address/Email**: user identifiers, useful for grouping & mismatch detection.  \n",
    "- **C, D, M features**: pre-computed stats on frequency, time deltas, matches.  \n",
    "- **V features**: anonymized engineered variables.  \n",
    "- **Identity features**: digital fingerprint of users.  \n",
    "\n",
    "Together, these 400+ features provide rich signals for modeling fraud detection."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2508f43",
   "metadata": {},
   "source": [
    "# Data quality check "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a808eca5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data quality check\n",
    "missing_values = df.isnull().sum()\n",
    "missing_percentage = (missing_values / len(df)) * 100\n",
    "missing_data = pd.DataFrame({'Missing Values': missing_values, 'Percentage': missing_percentage})\n",
    "missing_data = missing_data[missing_data['Missing Values'] > 0].sort_values(by='Missing Values', ascending=False)\n",
    "print(missing_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76a5e44e",
   "metadata": {},
   "source": [
    "# EDA on each group feature"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d08d40c4",
   "metadata": {},
   "source": [
    "## Transaction Features "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8db99257",
   "metadata": {},
   "source": [
    "### TransactionDT "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "913b4273",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Convert TransactionDT into days (approximate)\n",
    "df[\"TransactionDay\"] = df[\"TransactionDT\"] // (24*60*60)\n",
    "df[\"TransactionHour\"] = (df[\"TransactionDT\"] // 3600) % 24\n",
    "\n",
    "# Plot transactions over time\n",
    "plt.figure(figsize=(12,5))\n",
    "df[\"TransactionDay\"].plot(kind=\"hist\", bins=50, title=\"Distribution of Transaction Days\")\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(12,5))\n",
    "df[\"TransactionHour\"].plot(kind=\"hist\", bins=24, title=\"Distribution of Transaction Hours\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5efba22",
   "metadata": {},
   "source": [
    "The histogram shows a clear cyclical pattern in transaction time. Instead of being uniformly distributed, transactions are concentrated in specific time ranges, creating two main peaks. This indicates that transactions tend to occur more frequently during certain periods, possibly reflecting daily user behavior (example: daytime vs nighttime activity). Such patterns could be relevant in detecting anomalies, since fraudulent transactions may not follow the same temporal distribution as legitimate ones."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96361e6d",
   "metadata": {},
   "source": [
    "### Amount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a1a9934",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot amount distribution\n",
    "plt.figure(figsize=(12,5))\n",
    "df[\"TransactionAmt\"].plot(kind=\"hist\", bins=50, title=\"Distribution of Transaction Amounts\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f6146a4",
   "metadata": {},
   "source": [
    "The histogram shows that most transactions involve very small amounts, with the frequency rapidly decreasing as the transaction amount increases. This results in a highly skewed distribution with a long tail to the right. Only a small fraction of transactions involve large amounts, but these extreme values could be important when analyzing fraud, since abnormal or unusually high amounts might be associated with fraudulent behavior."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b86390a6",
   "metadata": {},
   "source": [
    "### Amount vs isFraud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e32a0992",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transaction Features Analysis\n",
    "\n",
    "# First, let's check the target variable distribution\n",
    "print(\"Target Variable Distribution:\")\n",
    "print(df['isFraud'].value_counts())\n",
    "print(f\"Fraud Rate: {df['isFraud'].mean():.4f}\")\n",
    "\n",
    "plt.figure(figsize=(8,5))\n",
    "df['isFraud'].value_counts().plot(kind='bar', title='Distribution of Fraud vs Non-Fraud')\n",
    "plt.xticks([0,1], ['Non-Fraud', 'Fraud'], rotation=0)\n",
    "plt.show()\n",
    "\n",
    "# Transaction Amount vs Fraud\n",
    "plt.figure(figsize=(12,6))\n",
    "plt.subplot(1,2,1)\n",
    "df[df['isFraud']==0]['TransactionAmt'].hist(bins=50, alpha=0.7, label='Non-Fraud', density=True)\n",
    "df[df['isFraud']==1]['TransactionAmt'].hist(bins=50, alpha=0.7, label='Fraud', density=True)\n",
    "plt.xlabel('Transaction Amount')\n",
    "plt.ylabel('Density')\n",
    "plt.legend()\n",
    "plt.title('Transaction Amount Distribution by Fraud Status')\n",
    "plt.yscale('log')\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "# Log transform for better visualization\n",
    "df['TransactionAmt_log'] = np.log1p(df['TransactionAmt'])\n",
    "df[df['isFraud']==0]['TransactionAmt_log'].hist(bins=50, alpha=0.7, label='Non-Fraud', density=True)\n",
    "df[df['isFraud']==1]['TransactionAmt_log'].hist(bins=50, alpha=0.7, label='Fraud', density=True)\n",
    "plt.xlabel('Log(Transaction Amount + 1)')\n",
    "plt.ylabel('Density')\n",
    "plt.legend()\n",
    "plt.title('Log Transaction Amount Distribution by Fraud Status')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Statistical summary by fraud status\n",
    "print(\"\\nTransaction Amount Statistics by Fraud Status:\")\n",
    "print(df.groupby('isFraud')['TransactionAmt'].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b48c5f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "sns.histplot(data=df, x=\"TransactionAmt_log\", hue=\"isFraud\", log_scale=False, kde=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20097347",
   "metadata": {},
   "source": [
    "- Log transformation is effective for reducing skewness and making the data more suitable for modeling.\n",
    "\n",
    "- Transaction amount alone is not a strong discriminator between Fraud and Non-Fraud, since their distributions are quite similar after transformation.\n",
    "\n",
    "- Very high-value transactions are mostly Non-Fraud, which could serve as a minor signal.\n",
    "\n",
    "- To detect fraud effectively, this feature should be combined with other behavioral or contextual features (time, frequency, location, sender/receiver, etc.)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01e00750",
   "metadata": {},
   "source": [
    "### ProductCD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3fc9087",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ProductCD Analysis\n",
    "plt.figure(figsize=(15,5))\n",
    "\n",
    "plt.subplot(1,3,1)\n",
    "df['ProductCD'].value_counts().plot(kind='bar', title='ProductCD Distribution')\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "plt.subplot(1,3,2)\n",
    "fraud_by_product = df.groupby('ProductCD')['isFraud'].mean()\n",
    "fraud_by_product.plot(kind='bar', title='Fraud Rate by ProductCD')\n",
    "plt.ylabel('Fraud Rate')\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "plt.subplot(1,3,3)\n",
    "product_fraud_counts = df.groupby(['ProductCD', 'isFraud']).size().unstack(fill_value=0)\n",
    "product_fraud_counts.plot(kind='bar', stacked=True, title='Fraud Count by ProductCD')\n",
    "plt.legend(['Non-Fraud', 'Fraud'])\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nFraud Rate by ProductCD:\")\n",
    "print(fraud_by_product.sort_values(ascending=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0840d3df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time-based Analysis\n",
    "# Transaction patterns by hour\n",
    "plt.figure(figsize=(15,5))\n",
    "\n",
    "plt.subplot(1,3,1)\n",
    "df['TransactionHour'].value_counts().sort_index().plot(kind='bar', title='Transactions by Hour')\n",
    "plt.xlabel('Hour of Day')\n",
    "\n",
    "plt.subplot(1,3,2)\n",
    "hourly_fraud_rate = df.groupby('TransactionHour')['isFraud'].mean()\n",
    "hourly_fraud_rate.plot(kind='bar', title='Fraud Rate by Hour')\n",
    "plt.ylabel('Fraud Rate')\n",
    "plt.xlabel('Hour of Day')\n",
    "\n",
    "plt.subplot(1,3,3)\n",
    "# Day of week analysis (approximate)\n",
    "df['DayOfWeek'] = df['TransactionDay'] % 7\n",
    "daily_fraud_rate = df.groupby('DayOfWeek')['isFraud'].mean()\n",
    "daily_fraud_rate.plot(kind='bar', title='Fraud Rate by Day of Week')\n",
    "plt.ylabel('Fraud Rate')\n",
    "plt.xlabel('Day of Week (0=Monday)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nFraud Rate by Hour:\")\n",
    "print(hourly_fraud_rate.sort_values(ascending=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58cf09d7",
   "metadata": {},
   "source": [
    "`1. Transactions by Hour (left)`\n",
    "\n",
    "Transaction volume is lowest during the early morning (3 AM â€“ 6 AM) and gradually increases during the day.\n",
    "\n",
    "There is a peak in the evening (around 6 PM â€“ 9 PM), when the number of transactions is the highest.\n",
    "\n",
    "This pattern likely reflects typical human activity: fewer transactions overnight and more during active business and leisure hours.\n",
    "\n",
    "`2. Fraud Rate by Hour (middle)`\n",
    "\n",
    "The fraud rate is highest in the early morning (around 6â€“9 AM), peaking sharply around 7 AM at more than 10%.\n",
    "\n",
    "After 10 AM, the fraud rate drops significantly and remains relatively stable throughout the day.\n",
    "\n",
    "Interestingly, fraud tends to occur disproportionately when transaction volume is low, which may indicate fraudsters exploit periods of lower monitoring or lower user activity.\n",
    "\n",
    "`3. Fraud Rate by Day of Week (right)`\n",
    "\n",
    "Fraud rates are relatively consistent across the week, but:\n",
    "\n",
    "Monday (0) and Wednesday (2) show the highest fraud rates (~3.7%).\n",
    "\n",
    "Friday (4) shows the lowest fraud rate (~3.1%).\n",
    "\n",
    "This suggests some weekly behavioral patterns, but the variation is not as strong as the hourly effect.\n",
    "\n",
    "`4. Key Takeaways`\n",
    "\n",
    "Fraudulent activity is time-sensitive: fraud risk is elevated during early-morning hours, despite fewer transactions overall.\n",
    "\n",
    "Monitoring systems may need extra vigilance in off-peak hours, when fraudsters seem more active.\n",
    "\n",
    "Fraud risk is slightly higher at the start and middle of the week, though differences across days are moderate.\n",
    "\n",
    "Combining transaction timing (hour, day) with other features could improve fraud detection models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d96c67c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deep analysis of Product, Hours, Amount vs Fraud\n",
    "import seaborn as sns\n",
    "\n",
    "plt.figure(figsize=(20, 15))\n",
    "\n",
    "# 1. Fraud Rate by Product and Hour (Heatmap)\n",
    "plt.subplot(3, 3, 1)\n",
    "fraud_heatmap = df.groupby(['ProductCD', 'TransactionHour'])['isFraud'].mean().unstack(fill_value=0)\n",
    "sns.heatmap(fraud_heatmap, annot=True, fmt='.3f', cmap='Reds', \n",
    "            cbar_kws={'label': 'Fraud Rate'})\n",
    "plt.title('Fraud Rate: Product vs Hour')\n",
    "plt.ylabel('Product')\n",
    "plt.xlabel('Hour')\n",
    "\n",
    "# 2. Transaction Count by Product and Hour\n",
    "plt.subplot(3, 3, 2)\n",
    "count_heatmap = df.groupby(['ProductCD', 'TransactionHour']).size().unstack(fill_value=0)\n",
    "sns.heatmap(count_heatmap, annot=True, fmt='d', cmap='Blues',\n",
    "            cbar_kws={'label': 'Transaction Count'})\n",
    "plt.title('Transaction Count: Product vs Hour')\n",
    "plt.ylabel('Product')\n",
    "plt.xlabel('Hour')\n",
    "\n",
    "# 3. Average Amount by Product and Hour\n",
    "plt.subplot(3, 3, 3)\n",
    "amount_heatmap = df.groupby(['ProductCD', 'TransactionHour'])['TransactionAmt'].mean().unstack(fill_value=0)\n",
    "sns.heatmap(amount_heatmap, annot=True, fmt='.0f', cmap='Greens',\n",
    "            cbar_kws={'label': 'Avg Amount'})\n",
    "plt.title('Average Amount: Product vs Hour')\n",
    "plt.ylabel('Product')\n",
    "plt.xlabel('Hour')\n",
    "\n",
    "# 4. Fraud Rate vs Transaction Amount by Product\n",
    "plt.subplot(3, 3, 4)\n",
    "for product in df['ProductCD'].unique():\n",
    "    if pd.notna(product):\n",
    "        product_data = df[df['ProductCD'] == product]\n",
    "        amount_bins = pd.cut(product_data['TransactionAmt'], bins=10)\n",
    "        fraud_by_amount = product_data.groupby(amount_bins)['isFraud'].mean()\n",
    "        plt.plot(range(len(fraud_by_amount)), fraud_by_amount.values, \n",
    "                marker='o', label=f'Product {product}')\n",
    "plt.title('Fraud Rate vs Amount by Product')\n",
    "plt.xlabel('Amount Bins (Low to High)')\n",
    "plt.ylabel('Fraud Rate')\n",
    "plt.legend()\n",
    "\n",
    "# 5. Fraud Rate vs Hour by Product\n",
    "plt.subplot(3, 3, 5)\n",
    "for product in df['ProductCD'].unique():\n",
    "    if pd.notna(product):\n",
    "        product_data = df[df['ProductCD'] == product]\n",
    "        hourly_fraud = product_data.groupby('TransactionHour')['isFraud'].mean()\n",
    "        plt.plot(hourly_fraud.index, hourly_fraud.values, \n",
    "                marker='o', label=f'Product {product}', linewidth=2)\n",
    "plt.title('Fraud Rate by Hour for Each Product')\n",
    "plt.xlabel('Hour')\n",
    "plt.ylabel('Fraud Rate')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# 6. Box plot: Amount distribution by Product and Fraud\n",
    "plt.subplot(3, 3, 6)\n",
    "df_sample = df.sample(10000)  # Sample for better visualization\n",
    "sns.boxplot(data=df_sample, x='ProductCD', y='TransactionAmt_log', hue='isFraud')\n",
    "plt.title('Amount Distribution by Product and Fraud')\n",
    "plt.ylabel('Log(Transaction Amount)')\n",
    "plt.yscale('linear')\n",
    "\n",
    "# 7. High-risk combinations (Product C + Hour 7)\n",
    "plt.subplot(3, 3, 7)\n",
    "risk_combinations = []\n",
    "fraud_rates = []\n",
    "labels = []\n",
    "\n",
    "for product in df['ProductCD'].unique():\n",
    "    if pd.notna(product):\n",
    "        for hour in [6, 7, 8]:  # Focus on high-risk hours\n",
    "            subset = df[(df['ProductCD'] == product) & (df['TransactionHour'] == hour)]\n",
    "            if len(subset) > 10:  # Only if enough samples\n",
    "                fraud_rate = subset['isFraud'].mean()\n",
    "                risk_combinations.append(f'{product}-H{hour}')\n",
    "                fraud_rates.append(fraud_rate)\n",
    "                labels.append(f'Product {product}, Hour {hour}')\n",
    "\n",
    "plt.bar(range(len(fraud_rates)), fraud_rates)\n",
    "plt.xticks(range(len(fraud_rates)), risk_combinations, rotation=45)\n",
    "plt.title('Fraud Rate for Product-Hour Combinations')\n",
    "plt.ylabel('Fraud Rate')\n",
    "\n",
    "# 8. Amount patterns for high-risk scenarios\n",
    "plt.subplot(3, 3, 8)\n",
    "high_risk = df[(df['ProductCD'] == 'C') & (df['TransactionHour'] == 7)]\n",
    "normal_risk = df[(df['ProductCD'] != 'C') & (df['TransactionHour'] != 7)]\n",
    "\n",
    "plt.hist([high_risk['TransactionAmt_log'], normal_risk['TransactionAmt_log']], \n",
    "         bins=30, alpha=0.7, density=True,\n",
    "         label=['High Risk (C+H7)', 'Normal Risk'])\n",
    "plt.title('Amount Distribution: High Risk vs Normal')\n",
    "plt.xlabel('Log(Transaction Amount)')\n",
    "plt.ylabel('Density')\n",
    "plt.legend()\n",
    "\n",
    "# 9. 3D visualization concept (simplified as scatter)\n",
    "plt.subplot(3, 3, 9)\n",
    "fraud_data = df[df['isFraud'] == 1].sample(min(1000, len(df[df['isFraud'] == 1])))\n",
    "normal_data = df[df['isFraud'] == 0].sample(min(1000, len(df[df['isFraud'] == 0])))\n",
    "\n",
    "plt.scatter(fraud_data['TransactionHour'], fraud_data['TransactionAmt_log'], \n",
    "           c='red', alpha=0.6, s=20, label='Fraud')\n",
    "plt.scatter(normal_data['TransactionHour'], normal_data['TransactionAmt_log'], \n",
    "           c='blue', alpha=0.3, s=10, label='Normal')\n",
    "plt.title('Hour vs Amount: Fraud vs Normal')\n",
    "plt.xlabel('Transaction Hour')\n",
    "plt.ylabel('Log(Transaction Amount)')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3d210c1",
   "metadata": {},
   "source": [
    "## Card feature "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bab1cd62",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = [f'card{i}' for i in range(1, 7)]\n",
    "\n",
    "for col in cols:\n",
    "    if col in df.columns:\n",
    "        print(f\"{col}: {df[col].nunique()} unique values\")\n",
    "        print(f'sample values: {df[col].dropna().unique()[:5]}')\n",
    "    else:\n",
    "        print(f\"{col} not in dataframe\") "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eea0b606",
   "metadata": {},
   "source": [
    "### card1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26af843c",
   "metadata": {},
   "outputs": [],
   "source": [
    "card1_counts = df['card1'].value_counts()\n",
    "\n",
    "plt.figure(figsize=(10,5))\n",
    "card1_counts[:20].plot(kind='bar', color='steelblue')\n",
    "plt.title(\"Top 20 card1 with Most Transactions\")\n",
    "plt.xlabel(\"card1 ID\")\n",
    "plt.ylabel(\"Number of Transactions\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91fbd771",
   "metadata": {},
   "source": [
    "After the top 2, the counts drop noticeably, showing a long-tail effect where most card IDs have far fewer transactions. \n",
    "\n",
    "7919 and 9500 stand out as extreme outliers with unusually high transaction activity. \n",
    "\n",
    "! These IDs could represent high-frequency users, automated transactions, or even anomalies worth deeper investigation (e.g., link to fraud ratio).\n",
    " \n",
    "`Such concentration can bias models: if fraud is concentrated in these high-activity IDs, they will disproportionately influence training.`\n",
    "\n",
    "`Important next step: check the fraud rate per card1, especially for IDs with extreme transaction volumes.`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99b2789c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "top4_data = df.groupby(['card1', 'isFraud']).size().unstack(fill_value=0).loc[card1_counts.index[:4]]\n",
    "\n",
    "fraud_ratio = (top4_data[1] / (top4_data[0] + top4_data[1]) * 100).round(2)\n",
    "\n",
    "ax = top4_data.plot(kind='bar', stacked=True, figsize=(8,6), color=['steelblue','tomato'])\n",
    "\n",
    "for container in ax.containers:\n",
    "    ax.bar_label(container, label_type='center', fontsize=9, color='white', fontweight='bold')\n",
    "\n",
    "for idx, val in enumerate(fraud_ratio):\n",
    "    total = top4_data.sum(axis=1).iloc[idx]\n",
    "    ax.text(idx, total + 200, f\"{val}%\", ha='center', va='bottom', fontsize=10, fontweight='bold', color='black')\n",
    "\n",
    "plt.title(\"Top 4 card1 with Most Transactions\")\n",
    "plt.xlabel(\"card1 ID\")\n",
    "plt.ylabel(\"Number of Transactions\")\n",
    "plt.legend(['Legit', 'Fraud'])\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c957053b",
   "metadata": {},
   "outputs": [],
   "source": [
    "K = 20\n",
    "topk_ids = df['card1'].value_counts().head(K).index\n",
    "\n",
    "g = (df[df['card1'].isin(topk_ids)]\n",
    "     .groupby('card1')['isFraud']\n",
    "     .agg(total='size', fraud='sum'))\n",
    "g['fraud_rate'] = g['fraud'] / g['total']\n",
    "\n",
    "g_bar = g.sort_values('fraud_rate', ascending=False)\n",
    "\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.bar(g_bar.index.astype(str), (g_bar['fraud_rate']*100).values)\n",
    "plt.title(f\"Fraud rate (%) - Top {K} card1 by transactions\")\n",
    "plt.xlabel(\"card1\")\n",
    "plt.ylabel(\"Fraud rate (%)\")\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(9,6))\n",
    "plt.scatter(g['total'], g['fraud_rate']*100)\n",
    "plt.title(f\"Total transactions vs Fraud rate (%) - Top {K} card1\")\n",
    "plt.xlabel(\"Total transactions\")\n",
    "plt.ylabel(\"Fraud rate (%)\")\n",
    "\n",
    "for rid, row in g.iterrows():\n",
    "    plt.annotate(str(rid), (row['total'], row['fraud_rate']*100),\n",
    "                 textcoords=\"offset points\", xytext=(4,4), fontsize=8)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9e29db3",
   "metadata": {},
   "source": [
    "- user 9633 has the highest fraud transaction "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c2449df",
   "metadata": {},
   "source": [
    "### card2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73c4d8e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "card2_counts = df['card2'].value_counts().head(20)\n",
    "\n",
    "plt.figure(figsize=(10,5))\n",
    "card2_counts.plot(kind='bar', color='teal')\n",
    "plt.title(\"Top 20 card2 (Bank IDs)\")\n",
    "plt.xlabel(\"card2\")\n",
    "plt.ylabel(\"Number of Transactions\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdc70d66",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "card2_stats = df.groupby('card2')['isFraud'].agg(total='count', fraud='sum')\n",
    "card2_stats['fraud_rate'] = card2_stats['fraud'] / card2_stats['total']\n",
    "\n",
    "card2_sorted = card2_stats.sort_values('fraud_rate', ascending=False).head(50)\n",
    "\n",
    "plt.figure(figsize=(12,6))\n",
    "plt.bar(card2_sorted.index.astype(str), card2_sorted['fraud_rate']*100)\n",
    "plt.title(\"Top 20 card2 with Highest Fraud Rate\")\n",
    "plt.xlabel(\"card2 (Bank ID)\")\n",
    "plt.ylabel(\"Fraud Rate (%)\")\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "353a9e52",
   "metadata": {},
   "source": [
    "The highest-risk banks (e.g., 289.0, 405.0) show fraud rates above 40%, which is extremely high compared to the baseline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24c8f7e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "fraud_pairs = (df.groupby(['card1','card2'])['isFraud']\n",
    "                 .agg(total='count', fraud='sum'))\n",
    "fraud_pairs['fraud_rate'] = fraud_pairs['fraud'] / fraud_pairs['total']\n",
    "fraud_pairs = fraud_pairs.reset_index()\n",
    "\n",
    "\n",
    "# top_pairs = fraud_pairs.sort_values('fraud', ascending=False).head(20)\n",
    "\n",
    "\n",
    "top_card1 = fraud_pairs.groupby('card1')['fraud'].sum().sort_values(ascending=False).head(10).index\n",
    "subset = fraud_pairs[fraud_pairs['card1'].isin(top_card1)]\n",
    "\n",
    "# pivot cho heatmap\n",
    "pivot = subset.pivot_table(values='fraud_rate', index='card1', columns='card2', fill_value=0)\n",
    "\n",
    "plt.figure(figsize=(14,6))\n",
    "sns.heatmap(pivot*100, cmap=\"Reds\", cbar_kws={'label': 'Fraud Rate (%)'})\n",
    "plt.title(\"Fraud Rate (%) by card1 & card2\")\n",
    "plt.xlabel(\"card2 (Bank ID)\")\n",
    "plt.ylabel(\"card1 (User ID)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaa0a8f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,6))\n",
    "sns.scatterplot(data=fraud_pairs, x='total', y='fraud_rate', hue='card2', alpha=0.7, legend=False)\n",
    "plt.title(\"Fraud Rate vs Total Transactions (by card1-card2)\")\n",
    "plt.xlabel(\"Total Transactions\")\n",
    "plt.ylabel(\"Fraud Rate\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8cb4087",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cec8b157",
   "metadata": {},
   "source": [
    "### card3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "468b525e",
   "metadata": {},
   "outputs": [],
   "source": [
    "card3_counts = df['card3'].value_counts().head(20) \n",
    "\n",
    "plt.figure(figsize=(12,6))\n",
    "sns.barplot(x=card3_counts.index.astype(str), y=card3_counts.values, color='steelblue')\n",
    "plt.title(\"Top 20 card3 with Most Transactions\")\n",
    "plt.xlabel(\"card3\")\n",
    "plt.ylabel(\"Number of Transactions\")\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7de7efb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "card3_stats = df.groupby('card3')['isFraud'].agg(total='count', fraud='sum')\n",
    "card3_stats['fraud_rate'] = card3_stats['fraud'] / card3_stats['total']\n",
    "\n",
    "# láº¥y top 20 theo fraud_rate\n",
    "card3_sorted = card3_stats.sort_values('fraud_rate', ascending=False).head(60)\n",
    "\n",
    "plt.figure(figsize=(12,6))\n",
    "sns.barplot(x=card3_sorted.index.astype(str), y=card3_sorted['fraud_rate']*100, color='tomato')\n",
    "plt.title(\"Top 20 card3 with Highest Fraud Rate\")\n",
    "plt.xlabel(\"card3\")\n",
    "plt.ylabel(\"Fraud Rate (%)\")\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9411acc0",
   "metadata": {},
   "source": [
    "There are many types of card3 that show a 100% fraud rate, but these categories have only a very small number of transactions. This means the high fraud rate is likely due to low sample size rather than a genuine risk pattern, and should be interpreted with caution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34c43cf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fraud_c23 = (df.groupby(['card2','card3'])['isFraud']\n",
    "               .mean()\n",
    "               .unstack(fill_value=0))\n",
    "\n",
    "plt.figure(figsize=(20,10))\n",
    "sns.heatmap(fraud_c23*100, cmap=\"Reds\", cbar_kws={'label': 'Fraud Rate (%)'})\n",
    "plt.title(\"Fraud Rate (%) by card2 (Bank) and card3 (Region)\")\n",
    "plt.xlabel(\"card3\")\n",
    "plt.ylabel(\"card2\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d14e1ca",
   "metadata": {},
   "source": [
    "### card4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bd37682",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plt.figure(figsize=(6,4))\n",
    "sns.countplot(x='card4', data=df, order=df['card4'].value_counts().index, palette=\"Set2\")\n",
    "plt.title(\"Distribution of Transactions by card4 (Card Brand)\")\n",
    "plt.xlabel(\"card4 (Brand)\")\n",
    "plt.ylabel(\"Number of Transactions\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "card4_stats = df.groupby('card4')['isFraud'].agg(total='count', fraud='sum')\n",
    "card4_stats['fraud_rate'] = card4_stats['fraud'] / card4_stats['total']\n",
    "\n",
    "plt.figure(figsize=(6,4))\n",
    "sns.barplot(x=card4_stats.index, y=card4_stats['fraud_rate']*100, palette=\"Set1\")\n",
    "plt.title(\"Fraud Rate (%) by card4 (Card Brand)\")\n",
    "plt.xlabel(\"card4 (Brand)\")\n",
    "plt.ylabel(\"Fraud Rate (%)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00e0980e",
   "metadata": {},
   "source": [
    "While Visa has the highest number of transactions, the Discover card shows the highest risk with the highest fraud rate. This indicates that although Visa dominates in volume, fraud detection efforts should pay particular attention to Discover, as it is disproportionately associated with fraudulent activity relative to its transaction count."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91159e61",
   "metadata": {},
   "source": [
    "### card6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e63a3280",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plt.figure(figsize=(6,4))\n",
    "sns.countplot(x='card6', data=df, order=df['card6'].value_counts().index, palette=\"Set2\")\n",
    "plt.title(\"Distribution of Transactions by card6 (Card Type)\")\n",
    "plt.xlabel(\"card6 (Type)\")\n",
    "plt.ylabel(\"Number of Transactions\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "card6_stats = df.groupby('card6')['isFraud'].agg(total='count', fraud='sum')\n",
    "card6_stats['fraud_rate'] = card6_stats['fraud'] / card6_stats['total']\n",
    "\n",
    "plt.figure(figsize=(6,4))\n",
    "sns.barplot(x=card6_stats.index, y=card6_stats['fraud_rate']*100, palette=\"Set1\")\n",
    "plt.title(\"Fraud Rate (%) by card6 (Card Type)\")\n",
    "plt.xlabel(\"card6 (Type)\")\n",
    "plt.ylabel(\"Fraud Rate (%)\")\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4fefb45",
   "metadata": {},
   "source": [
    "## Address features "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4773c3c",
   "metadata": {},
   "source": [
    "### P_emaildomain "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ae52ff2",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxplot(data=df, x='P_emaildomain', hue='isFraud', palette=\"Set3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7612a860",
   "metadata": {},
   "outputs": [],
   "source": [
    "p_email_counts = df['P_emaildomain'].value_counts().head(50)\n",
    "\n",
    "plt.figure(figsize=(10,5))\n",
    "sns.barplot(x=p_email_counts.index, y=p_email_counts.values, color='steelblue')\n",
    "plt.title(\"Top 10 Purchaser Email Domains (P_emaildomain)\")\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.ylabel(\"Number of Transactions\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46b950c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "p_email_stats = df.groupby('P_emaildomain')['isFraud'].agg(total='count', fraud='sum')\n",
    "p_email_stats['fraud_rate'] = p_email_stats['fraud'] / p_email_stats['total']\n",
    "p_email_stats.sort_values('fraud_rate', ascending=False).head(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b08c8db9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "p_email_stats = df.groupby('P_emaildomain')['isFraud'].agg(total='count', fraud='sum')\n",
    "p_email_stats['fraud_rate'] = p_email_stats['fraud'] / p_email_stats['total']\n",
    "p_email_stats = p_email_stats.sort_values('total', ascending=False).head(200)  \n",
    "\n",
    "fig, ax1 = plt.subplots(figsize=(20,10))\n",
    "\n",
    "sns.barplot(x=p_email_stats.index, y=p_email_stats['total'], color='steelblue', ax=ax1)\n",
    "ax1.set_ylabel(\"Number of Transactions\", color=\"steelblue\")\n",
    "ax1.set_xlabel(\"Purchaser Email Domain\")\n",
    "ax1.set_xticklabels(p_email_stats.index, rotation=45, ha='right')\n",
    "\n",
    "ax2 = ax1.twinx()\n",
    "sns.lineplot(x=p_email_stats.index, y=p_email_stats['fraud_rate']*100, \n",
    "             marker='o', sort=False, color='tomato', ax=ax2)\n",
    "ax2.set_ylabel(\"Fraud Rate (%)\", color=\"tomato\")\n",
    "\n",
    "plt.title(\"Purchaser Email Domains: Transaction Volume & Fraud Rate\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "298e96ef",
   "metadata": {},
   "source": [
    "The figure shows that protonmail.com has the highest observed fraud rate (around 40%), but this is based on a very small number of transactions, which may introduce bias. Similarly, domains such as mail.com, outlook.es, and aim.com record fraud rates above 15%, yet their sample sizes are also limited, so these results should be interpreted with caution.\n",
    "\n",
    "In contrast, major â€œbig techâ€ domains like gmail.com, yahoo.com, and hotmail.com account for the majority of transactions and exhibit fraud rates below 10%. This suggests that while fraud does occur across all email providers, rare or niche domains tend to show disproportionately higher fraud risk, although their impact on the overall dataset is smaller due to low transaction volume."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29c9f242",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "top5_domains = df['P_emaildomain'].value_counts().head(5).index.tolist()\n",
    "\n",
    "def categorize_email(domain):\n",
    "    if pd.isnull(domain):\n",
    "        return \"missing\"\n",
    "    domain = domain.lower()\n",
    "    if domain in top5_domains:\n",
    "        return \"BigTech_Top5\"\n",
    "    else:\n",
    "        return \"Others\"\n",
    "\n",
    "df['email_group2'] = df['P_emaildomain'].apply(categorize_email)\n",
    "\n",
    "email_stats2 = df.groupby('email_group2')['isFraud'].agg(total='count', fraud='sum')\n",
    "email_stats2['fraud_rate'] = email_stats2['fraud'] / email_stats2['total']\n",
    "\n",
    "print(email_stats2)\n",
    "\n",
    "plt.figure(figsize=(6,4))\n",
    "sns.barplot(x=email_stats2.index, y=email_stats2['fraud_rate']*100, palette=\"Set2\")\n",
    "plt.title(\"Fraud Rate (%) by Email Group (Top 5 vs Others)\")\n",
    "plt.xlabel(\"Email Group\")\n",
    "plt.ylabel(\"Fraud Rate (%)\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d2f89cd",
   "metadata": {},
   "source": [
    "-> can use this like feature engineering -> reduce bias because small samples size , learn pattern more ez"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40a20c6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "email_group_stats2 = df.groupby(['email_group2','isFraud']).size().unstack(fill_value=0)\n",
    "email_group_stats2.columns = ['Legit','Fraud']\n",
    "\n",
    "ax = email_group_stats2.plot(kind='bar', stacked=True, figsize=(8,6), \n",
    "                             color=['steelblue','tomato'])\n",
    "\n",
    "for container in ax.containers:\n",
    "    ax.bar_label(container, label_type='center', color='white', fontsize=9, fontweight='bold')\n",
    "\n",
    "plt.title(\"Transaction Distribution by Email Group (Top 5 vs Others)\")\n",
    "plt.xlabel(\"Email Group\")\n",
    "plt.ylabel(\"Number of Transactions\")\n",
    "plt.legend([\"Legit\",\"Fraud\"])\n",
    "plt.xticks(rotation=0)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fe8aab1",
   "metadata": {},
   "source": [
    "### R_emaildomain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d7ad038",
   "metadata": {},
   "outputs": [],
   "source": [
    "p_email_counts = df['R_emaildomain'].value_counts().head(50)\n",
    "\n",
    "plt.figure(figsize=(10,5))\n",
    "sns.barplot(x=p_email_counts.index, y=p_email_counts.values, color='steelblue')\n",
    "plt.title(\"Top 10 Purchaser Email Domains (R_emaildomain)\")\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.ylabel(\"Number of Transactions\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd7dba02",
   "metadata": {},
   "outputs": [],
   "source": [
    "r_email_stats = df.groupby('R_emaildomain')['isFraud'].agg(total='count', fraud='sum')\n",
    "r_email_stats['fraud_rate'] = r_email_stats['fraud'] / r_email_stats['total']\n",
    "r_email_stats.sort_values('fraud_rate', ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edf9974c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "r_email_stats = df.groupby('R_emaildomain')['isFraud'].agg(total='count', fraud='sum')\n",
    "r_email_stats['fraud_rate'] = r_email_stats['fraud'] / r_email_stats['total']\n",
    "r_email_stats = r_email_stats.sort_values('total', ascending=False).head(200)\n",
    "\n",
    "fig, ax1 = plt.subplots(figsize=(20,10))\n",
    "\n",
    "sns.barplot(x=r_email_stats.index, y=r_email_stats['total'], color='steelblue', ax=ax1)\n",
    "ax1.set_ylabel(\"Number of Transactions\", color=\"steelblue\")\n",
    "ax1.set_xlabel(\"Receiver Email Domain\")\n",
    "ax1.set_xticklabels(r_email_stats.index, rotation=45, ha='right')\n",
    "\n",
    "ax2 = ax1.twinx()\n",
    "sns.lineplot(x=r_email_stats.index, y=r_email_stats['fraud_rate']*100, \n",
    "             marker='o', sort=False, color='tomato', ax=ax2)\n",
    "ax2.set_ylabel(\"Fraud Rate (%)\", color=\"tomato\")\n",
    "\n",
    "plt.title(\"Receiver Email Domains: Transaction Volume & Fraud Rate\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99a1bd38",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "top5_domains_r = df['R_emaildomain'].value_counts().head(5).index.tolist()\n",
    "\n",
    "def categorize_email_r(domain):\n",
    "    if pd.isnull(domain):\n",
    "        return \"missing\"\n",
    "    domain = domain.lower()\n",
    "    if domain in top5_domains_r:\n",
    "        return \"BigTech_Top5\"\n",
    "    else:\n",
    "        return \"Others\"\n",
    "\n",
    "df['email_group2_R'] = df['R_emaildomain'].apply(categorize_email_r)\n",
    "\n",
    "email_stats2_r = df.groupby('email_group2_R')['isFraud'].agg(total='count', fraud='sum')\n",
    "email_stats2_r['fraud_rate'] = email_stats2_r['fraud'] / email_stats2_r['total']\n",
    "\n",
    "print(email_stats2_r)\n",
    "\n",
    "plt.figure(figsize=(6,4))\n",
    "sns.barplot(x=email_stats2_r.index, y=email_stats2_r['fraud_rate']*100, palette=\"Set2\")\n",
    "plt.title(\"Fraud Rate (%) by R_emaildomain Group (Top 5 vs Others)\")\n",
    "plt.xlabel(\"Email Group (Recipient)\")\n",
    "plt.ylabel(\"Fraud Rate (%)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b480ff1",
   "metadata": {},
   "outputs": [],
   "source": [
    "email_group_stats2_r = df.groupby(['email_group2_R','isFraud']).size().unstack(fill_value=0)\n",
    "email_group_stats2_r.columns = ['Legit','Fraud']\n",
    "\n",
    "ax = email_group_stats2_r.plot(kind='bar', stacked=True, figsize=(8,6), \n",
    "                               color=['steelblue','tomato'])\n",
    "\n",
    "for container in ax.containers:\n",
    "    ax.bar_label(container, label_type='center', color='white', fontsize=9, fontweight='bold')\n",
    "\n",
    "plt.title(\"Transaction Distribution by R_emaildomain Group (Top 5 vs Others)\")\n",
    "plt.xlabel(\"Recipient Email Group\")\n",
    "plt.ylabel(\"Number of Transactions\")\n",
    "plt.legend([\"Legit\",\"Fraud\"])\n",
    "plt.xticks(rotation=0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1510c9d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['email_match'] = (df['P_emaildomain'] == df['R_emaildomain']).astype(int)\n",
    "\n",
    "\n",
    "email_match_stats = df.groupby('email_match')['isFraud'].agg(total='count', fraud='sum')\n",
    "email_match_stats['fraud_rate'] = email_match_stats['fraud'] / email_match_stats['total']\n",
    "print(email_match_stats)\n",
    "\n",
    "plt.figure(figsize=(6,4))\n",
    "sns.barplot(x=email_match_stats.index, y=email_match_stats['fraud_rate']*100, palette=\"Set1\")\n",
    "plt.xticks([0,1], [\"Mismatch\",\"Match\"])\n",
    "plt.title(\"Fraud Rate by Email Domain Match vs Mismatch\")\n",
    "plt.ylabel(\"Fraud Rate (%)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4d96779",
   "metadata": {},
   "outputs": [],
   "source": [
    "top5_domains = df['P_emaildomain'].value_counts().head(5).index.tolist()\n",
    "\n",
    "def categorize_email(domain):\n",
    "    if pd.isnull(domain):\n",
    "        return \"missing\"\n",
    "    domain = domain.lower()\n",
    "    if domain in top5_domains:\n",
    "        return \"BigTech_Top5\"\n",
    "    else:\n",
    "        return \"Others\"\n",
    "\n",
    "df['P_group'] = df['P_emaildomain'].apply(categorize_email)\n",
    "df['R_group'] = df['R_emaildomain'].apply(categorize_email)\n",
    "\n",
    "cross_tab = pd.crosstab(df['P_group'], df['R_group'], \n",
    "                        values=df['isFraud'], aggfunc='mean').fillna(0) * 100\n",
    "\n",
    "plt.figure(figsize=(6,4))\n",
    "sns.heatmap(cross_tab, annot=True, fmt=\".2f\", cmap=\"Reds\")\n",
    "plt.title(\"Fraud Rate (%) by P_emaildomain vs R_emaildomain Group\")\n",
    "plt.ylabel(\"Purchaser Email Group\")\n",
    "plt.xlabel(\"Recipient Email Group\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9760394",
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_tab_counts = pd.crosstab(df['P_group'], df['R_group'])\n",
    "cross_tab_rate = pd.crosstab(df['P_group'], df['R_group'], \n",
    "                             values=df['isFraud'], aggfunc='mean').fillna(0) * 100\n",
    "\n",
    "\n",
    "cross_tab_log = np.log1p(cross_tab_counts) \n",
    "plt.figure(figsize=(6,4))\n",
    "sns.heatmap(cross_tab_log, annot=True, fmt=\".1f\", cmap=\"Blues\")\n",
    "plt.title(\"Log(Transactions) by P_emaildomain vs R_emaildomain Group\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d76f81fd",
   "metadata": {},
   "source": [
    "## Counting feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf07e864",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75e257a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "counting_df = df[['C1', 'C2', 'C3', 'C4', 'C5', 'C6', 'C7', 'C8', 'C9', 'C10', 'C11', 'C12', 'C13', 'C14' , 'isFraud']] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b64e263",
   "metadata": {},
   "outputs": [],
   "source": [
    "# heatmap of correlations\n",
    "\n",
    "plt.figure(figsize=(12,8))\n",
    "sns.heatmap(counting_df.corr(), annot=True, fmt=\".2f\", cmap=\"coolwarm\", cbar_kws={'label': 'Correlation'})\n",
    "plt.title(\"Correlation Heatmap of C1 to C14 Features\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6b25ac5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# scatter plot of c1 , c2 with isfraud\n",
    "sns.relplot(data= counting_df , x= 'C1' , y= 'C2' ,hue= 'isFraud') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ac8564e",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.relplot(data= counting_df , x= 'C1' , y= 'C3' ,hue= 'isFraud') "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12277b58",
   "metadata": {},
   "source": [
    "### PCA - Visualize data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b70abe77",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Prepare the data\n",
    "X = df[['C1','C2','C3','C4','C5','C6','C7','C8','C9','C10','C11','C12','C13','C14']]\n",
    "feature_names = X.columns\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Fit PCA\n",
    "pca = PCA(n_components=14)  # All components to see full picture\n",
    "X_pca = pca.fit_transform(X_scaled)\n",
    "\n",
    "# Create DataFrame to track contributions\n",
    "components_df = pd.DataFrame(\n",
    "    pca.components_.T,  # Transpose to have features as rows\n",
    "    columns=[f'PC{i+1}' for i in range(pca.n_components_)],\n",
    "    index=feature_names\n",
    ")\n",
    "\n",
    "print(\"=== PCA COMPONENT ANALYSIS ===\\n\")\n",
    "\n",
    "# Show explained variance ratio\n",
    "print(\"Explained Variance Ratio by Component:\")\n",
    "for i, ratio in enumerate(pca.explained_variance_ratio_):\n",
    "    print(f\"PC{i+1}: {ratio:.4f} ({ratio*100:.2f}%)\")\n",
    "\n",
    "print(f\"\\nCumulative explained variance (first 3 components): {pca.explained_variance_ratio_[:3].sum():.4f}\")\n",
    "\n",
    "# Show top contributing features for each component\n",
    "print(\"\\n=== TOP CONTRIBUTING FEATURES FOR EACH COMPONENT ===\")\n",
    "for i in range(min(5, pca.n_components_)):  # Show first 5 components\n",
    "    pc_name = f'PC{i+1}'\n",
    "    print(f\"\\n{pc_name} (explains {pca.explained_variance_ratio_[i]*100:.2f}% variance):\")\n",
    "    \n",
    "    # Get absolute values and sort\n",
    "    contributions = components_df[pc_name].abs().sort_values(ascending=False)\n",
    "    \n",
    "    print(\"Top 5 contributing features:\")\n",
    "    for j, (feature, contribution) in enumerate(contributions.head(5).items()):\n",
    "        original_value = components_df.loc[feature, pc_name]  # Get original sign\n",
    "        print(f\"  {j+1}. {feature}: {original_value:.4f} (|{contribution:.4f}|)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddd07c45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize component contributions\n",
    "plt.figure(figsize=(16, 12))\n",
    "\n",
    "# Plot 1: Heatmap of all components\n",
    "plt.subplot(2, 2, 1)\n",
    "sns.heatmap(components_df.iloc[:, :8].T, annot=True, fmt='.3f', cmap='RdBu_r', center=0,\n",
    "            cbar_kws={'label': 'Component Loading'})\n",
    "plt.title('PCA Component Loadings (First 8 Components)')\n",
    "plt.xlabel('Original Features')\n",
    "plt.ylabel('Principal Components')\n",
    "\n",
    "# Plot 2: Bar plot for PC1\n",
    "plt.subplot(2, 2, 2)\n",
    "pc1_contributions = components_df['PC1'].sort_values(key=abs, ascending=False)\n",
    "colors = ['red' if x < 0 else 'blue' for x in pc1_contributions]\n",
    "plt.bar(range(len(pc1_contributions)), pc1_contributions.values, color=colors, alpha=0.7)\n",
    "plt.xticks(range(len(pc1_contributions)), pc1_contributions.index, rotation=45)\n",
    "plt.title(f'PC1 Feature Contributions (Explains {pca.explained_variance_ratio_[0]*100:.1f}%)')\n",
    "plt.ylabel('Loading')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 3: Bar plot for PC2\n",
    "plt.subplot(2, 2, 3)\n",
    "pc2_contributions = components_df['PC2'].sort_values(key=abs, ascending=False)\n",
    "colors = ['red' if x < 0 else 'blue' for x in pc2_contributions]\n",
    "plt.bar(range(len(pc2_contributions)), pc2_contributions.values, color=colors, alpha=0.7)\n",
    "plt.xticks(range(len(pc2_contributions)), pc2_contributions.index, rotation=45)\n",
    "plt.title(f'PC2 Feature Contributions (Explains {pca.explained_variance_ratio_[1]*100:.1f}%)')\n",
    "plt.ylabel('Loading')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 4: Cumulative explained variance\n",
    "plt.subplot(2, 2, 4)\n",
    "cumvar = np.cumsum(pca.explained_variance_ratio_)\n",
    "plt.plot(range(1, len(cumvar)+1), cumvar, 'bo-')\n",
    "plt.xlabel('Number of Components')\n",
    "plt.ylabel('Cumulative Explained Variance')\n",
    "plt.title('Cumulative Explained Variance')\n",
    "plt.grid(True, alpha=0.3)\n",
    "# Add lines for common thresholds\n",
    "plt.axhline(y=0.8, color='r', linestyle='--', alpha=0.7, label='80%')\n",
    "plt.axhline(y=0.95, color='g', linestyle='--', alpha=0.7, label='95%')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d086702",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detailed analysis for the first 3 components (most important)\n",
    "print(\"\\n=== DETAILED ANALYSIS OF TOP 3 COMPONENTS ===\")\n",
    "\n",
    "for i in range(3):\n",
    "    pc_name = f'PC{i+1}'\n",
    "    print(f\"\\n{'-'*50}\")\n",
    "    print(f\"{pc_name} - Explains {pca.explained_variance_ratio_[i]*100:.2f}% of variance\")\n",
    "    print(f\"{'-'*50}\")\n",
    "    \n",
    "    # All contributions for this component\n",
    "    contributions = components_df[pc_name].sort_values(key=abs, ascending=False)\n",
    "    \n",
    "    print(\"All feature contributions (sorted by absolute value):\")\n",
    "    for feature, contribution in contributions.items():\n",
    "        print(f\"  {feature}: {contribution:+.4f}\")\n",
    "    \n",
    "    # Interpretation\n",
    "    positive_features = contributions[contributions > 0.2].index.tolist()\n",
    "    negative_features = contributions[contributions < -0.2].index.tolist()\n",
    "    \n",
    "    if positive_features:\n",
    "        print(f\"\\nStrongly positive features (>0.2): {positive_features}\")\n",
    "    if negative_features:\n",
    "        print(f\"Strongly negative features (<-0.2): {negative_features}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "146e7f59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a summary table for easy reference\n",
    "summary_table = pd.DataFrame()\n",
    "\n",
    "for i in range(min(5, pca.n_components_)):\n",
    "    pc_name = f'PC{i+1}'\n",
    "    \n",
    "    # Get top 3 positive and negative contributors\n",
    "    contributions = components_df[pc_name].sort_values(ascending=False)\n",
    "    \n",
    "    top_positive = contributions.head(3)\n",
    "    top_negative = contributions.tail(3)\n",
    "    \n",
    "    summary_table[f'{pc_name}_Positive'] = [f\"{idx}: {val:.3f}\" for idx, val in top_positive.items()]\n",
    "    summary_table[f'{pc_name}_Negative'] = [f\"{idx}: {val:.3f}\" for idx, val in top_negative.items()]\n",
    "\n",
    "summary_table.index = ['1st', '2nd', '3rd']\n",
    "\n",
    "print(\"\\n=== SUMMARY TABLE: TOP CONTRIBUTORS BY COMPONENT ===\")\n",
    "print(summary_table)\n",
    "\n",
    "# Save component information for later use\n",
    "components_info = {\n",
    "    'explained_variance_ratio': pca.explained_variance_ratio_,\n",
    "    'components_df': components_df,\n",
    "    'feature_names': feature_names,\n",
    "    'cumulative_variance': np.cumsum(pca.explained_variance_ratio_)\n",
    "}\n",
    "\n",
    "print(f\"\\n=== KEY INSIGHTS ===\")\n",
    "print(f\"â€¢ First 2 components explain {pca.explained_variance_ratio_[:2].sum()*100:.1f}% of variance\")\n",
    "print(f\"â€¢ First 3 components explain {pca.explained_variance_ratio_[:3].sum()*100:.1f}% of variance\")\n",
    "print(f\"â€¢ Need {np.argmax(np.cumsum(pca.explained_variance_ratio_) >= 0.8) + 1} components for 80% variance\")\n",
    "print(f\"â€¢ Need {np.argmax(np.cumsum(pca.explained_variance_ratio_) >= 0.95) + 1} components for 95% variance\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eb83155",
   "metadata": {},
   "source": [
    "## Time Delta features "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4f65caf",
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in ['D1', 'D2', 'D3', 'D4', 'D5', 'D6', 'D7', 'D8', 'D9' , 'D10', 'D11', 'D12', 'D13', 'D14' , 'D15']:\n",
    "    if col in df.columns:\n",
    "        print(f\"{col}: {df[col].nunique()} unique values\")\n",
    "        print(f'sample values: {df[col].dropna().unique()[:5]}')\n",
    "    else:\n",
    "        print(f\"{col} not in dataframe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08c6ad54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution of D1 to D15 \n",
    "d_cols = ['D1', 'D2', 'D3', 'D4', 'D5', 'D6', 'D7', 'D8', 'D9' , 'D10', 'D11', 'D12', 'D13', 'D14' , 'D15']\n",
    "plt.figure(figsize=(20, 15))\n",
    "for i, col in enumerate(d_cols):\n",
    "    if col in df.columns:\n",
    "        plt.subplot(4, 4, i+1)\n",
    "        sns.histplot(df[col].dropna(), bins=30, kde=True, color='steelblue')\n",
    "        plt.title(f'Distribution of {col}')\n",
    "        plt.xlabel(col)\n",
    "        plt.ylabel('Frequency')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45f45813",
   "metadata": {},
   "outputs": [],
   "source": [
    "# boxplot of D1 to D15 by isFraud\n",
    "plt.figure(figsize=(20, 15))\n",
    "for i, col in enumerate(d_cols):\n",
    "    if col in df.columns:\n",
    "        plt.subplot(4, 4, i+1)\n",
    "        sns.boxplot(data=df, x='isFraud', y=col, palette=\"Set2\")\n",
    "        plt.title(f'{col} by Fraud Status')\n",
    "        plt.xlabel('isFraud')\n",
    "        plt.ylabel(col)\n",
    "        plt.yscale('linear')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "373458ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# correlation heatmap of D1 to D15\n",
    "plt.figure(figsize=(12,8))\n",
    "sns.heatmap(df[d_cols].corr(), annot=True, fmt=\".2f\", cmap=\"coolwarm\", cbar_kws={'label': 'Correlation'})\n",
    "plt.title(\"Correlation Heatmap of D1 to D15 Features\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7a7be36",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  scattrer plot of D1 vs D2 colored by isFraud\n",
    "plt.figure(figsize=(8,6))   \n",
    "sns.scatterplot(data=df, x='D1', y='D2', hue='isFraud', alpha=0.5, palette={0:'blue', 1:'red'})\n",
    "plt.title(\"D1 vs D2 Colored by Fraud Status\")\n",
    "plt.xlabel(\"D1\")\n",
    "plt.ylabel(\"D2\")\n",
    "plt.yscale('linear')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2827036b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# scatter plot of D6 vs D4 colored by isFraud\n",
    "plt.figure(figsize=(8,6))\n",
    "sns.scatterplot(data=df, x='D6', y='D4', hue='isFraud', alpha=0.5, palette={0:'blue', 1:'red'})\n",
    "plt.title(\"D6 vs D4 Colored by Fraud Status\")\n",
    "plt.xlabel(\"D6\")\n",
    "plt.ylabel(\"D4\")\n",
    "plt.yscale('linear')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5434ff48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# scatter plot of D6 vs D12 colored by isFraud\n",
    "plt.figure(figsize=(8,6))\n",
    "sns.scatterplot(data=df, x='D6', y='D12', hue='isFraud', alpha=0.5, palette={0:'blue', 1:'red'})\n",
    "plt.title(\"D6 vs D12 Colored by Fraud Status\")\n",
    "plt.xlabel(\"D6\")\n",
    "plt.ylabel(\"D12\")\n",
    "plt.yscale('linear')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd225096",
   "metadata": {},
   "outputs": [],
   "source": [
    "# scatter plot of D12 vs D4 colored by isFraud\n",
    "plt.figure(figsize=(8,6))\n",
    "sns.scatterplot(data=df, x='D12', y='D4', hue='isFraud', alpha=0.5, palette={0:'blue', 1:'red'})\n",
    "plt.title(\"D12 vs D4 Colored by Fraud Status\")\n",
    "plt.xlabel(\"D12\")\n",
    "plt.ylabel(\"D4\")\n",
    "plt.yscale('linear')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5b18aa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# scatter plot of D4 and D5 colored by isFraud\n",
    "plt.figure(figsize=(8,6))\n",
    "sns.scatterplot(data=df, x='D4', y='D5', hue='isFraud', alpha=0.5, palette={0:'blue', 1:'red'})\n",
    "plt.title(\"D4 vs D5 Colored by Fraud Status\")\n",
    "plt.xlabel(\"D4\")\n",
    "plt.ylabel(\"D5\")\n",
    "plt.yscale('linear')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81b1ae72",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "# Prepare Time Delta data\n",
    "d_cols = ['D1', 'D2', 'D3', 'D4', 'D5', 'D6', 'D7', 'D8', 'D9', 'D10', 'D11', 'D12', 'D13', 'D14', 'D15']\n",
    "X_timedelta = df[d_cols].copy()\n",
    "\n",
    "print(\"=== TIME DELTA FEATURES ANALYSIS ===\\n\")\n",
    "print(\"Original shape:\", X_timedelta.shape)\n",
    "print(\"Missing values per feature:\")\n",
    "print(X_timedelta.isnull().sum())\n",
    "print()\n",
    "\n",
    "# Handle missing values - important for time delta features\n",
    "imputer = SimpleImputer(strategy='median')  # median better for time features\n",
    "X_timedelta_imputed = pd.DataFrame(\n",
    "    imputer.fit_transform(X_timedelta), \n",
    "    columns=d_cols,\n",
    "    index=X_timedelta.index\n",
    ")\n",
    "\n",
    "# Standardize the data\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X_timedelta_imputed)\n",
    "\n",
    "print(\"Data after preprocessing:\")\n",
    "print(\"Shape:\", X_scaled.shape)\n",
    "print(\"No missing values:\", not np.isnan(X_scaled).any())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c22108bb",
   "metadata": {},
   "outputs": [],
   "source": [
    " \n",
    "\n",
    "print(\"=== PCA RESULTS FOR TIME DELTA FEATURES ===\\n\")\n",
    "print(\"Explained Variance Ratio:\")\n",
    "for i, ratio in enumerate(pca.explained_variance_ratio_):\n",
    "    print(f\"PC{i+1}: {ratio:.4f} ({ratio*100:.2f}%)\")\n",
    "\n",
    "cumvar = np.cumsum(pca.explained_variance_ratio_)\n",
    "print(f\"\\nCumulative variance:\")\n",
    "print(f\"First 3 components: {cumvar[2]:.4f} ({cumvar[2]*100:.1f}%)\")\n",
    "print(f\"First 5 components: {cumvar[4]:.4f} ({cumvar[4]*100:.1f}%)\")\n",
    "\n",
    "# Show top contributors for first 3 components\n",
    "print(\"\\n=== TOP CONTRIBUTING FEATURES ===\")\n",
    "for i in range(3):\n",
    "    pc_name = f'PC{i+1}'\n",
    "    contributions = components_df[pc_name].abs().sort_values(ascending=False)\n",
    "    print(f\"\\n{pc_name} (explains {pca.explained_variance_ratio_[i]*100:.2f}%):\")\n",
    "    for j, (feature, contribution) in enumerate(contributions.head(5).items()):\n",
    "        original_value = components_df.loc[feature, pc_name]\n",
    "        print(f\"  {j+1}. {feature}: {original_value:+.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d79fc8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Visualize PCA Results\n",
    "plt.figure(figsize=(16, 12))\n",
    "\n",
    "# Plot 1: Scree plot\n",
    "plt.subplot(2, 3, 1)\n",
    "plt.plot(range(1, len(pca.explained_variance_ratio_)+1), pca.explained_variance_ratio_, 'bo-')\n",
    "plt.xlabel('Component')\n",
    "plt.ylabel('Explained Variance Ratio')\n",
    "plt.title('Scree Plot - Time Delta Features')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 2: Cumulative variance\n",
    "plt.subplot(2, 3, 2)\n",
    "plt.plot(range(1, len(cumvar)+1), cumvar, 'ro-')\n",
    "plt.axhline(y=0.8, color='g', linestyle='--', alpha=0.7, label='80%')\n",
    "plt.axhline(y=0.95, color='b', linestyle='--', alpha=0.7, label='95%')\n",
    "plt.xlabel('Number of Components')\n",
    "plt.ylabel('Cumulative Explained Variance')\n",
    "plt.title('Cumulative Variance')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 3: Heatmap of loadings\n",
    "plt.subplot(2, 3, 3)\n",
    "sns.heatmap(components_df.iloc[:, :8].T, annot=True, fmt='.2f', cmap='RdBu_r', center=0)\n",
    "plt.title('PCA Loadings (First 8 Components)')\n",
    "plt.xlabel('Time Delta Features')\n",
    "\n",
    "# Plot 4: PC1 vs PC2 scatter\n",
    "plt.subplot(2, 3, 4)\n",
    "scatter = plt.scatter(X_pca[:, 0], X_pca[:, 1], c=df['isFraud'], cmap='viridis', alpha=0.6)\n",
    "plt.xlabel(f'PC1 ({pca.explained_variance_ratio_[0]*100:.1f}%)')\n",
    "plt.ylabel(f'PC2 ({pca.explained_variance_ratio_[1]*100:.1f}%)')\n",
    "plt.title('PC1 vs PC2 (colored by Fraud)')\n",
    "plt.colorbar(scatter)\n",
    "\n",
    "# Plot 5: PC1 contributions\n",
    "plt.subplot(2, 3, 5)\n",
    "pc1_contrib = components_df['PC1'].sort_values(key=abs, ascending=False)\n",
    "colors = ['red' if x < 0 else 'blue' for x in pc1_contrib]\n",
    "plt.bar(range(len(pc1_contrib)), pc1_contrib.values, color=colors, alpha=0.7)\n",
    "plt.xticks(range(len(pc1_contrib)), pc1_contrib.index, rotation=45)\n",
    "plt.title('PC1 Feature Contributions')\n",
    "plt.ylabel('Loading')\n",
    "\n",
    "# Plot 6: PC2 contributions  \n",
    "plt.subplot(2, 3, 6)\n",
    "pc2_contrib = components_df['PC2'].sort_values(key=abs, ascending=False)\n",
    "colors = ['red' if x < 0 else 'blue' for x in pc2_contrib]\n",
    "plt.bar(range(len(pc2_contrib)), pc2_contrib.values, color=colors, alpha=0.7)\n",
    "plt.xticks(range(len(pc2_contrib)), pc2_contrib.index, rotation=45)\n",
    "plt.title('PC2 Feature Contributions')\n",
    "plt.ylabel('Loading')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca42f02c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab154292",
   "metadata": {},
   "source": [
    "## Matching Feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc786283",
   "metadata": {},
   "outputs": [],
   "source": [
    "# matching feature value \n",
    "\n",
    "matching_cols = ['M1', 'M2', 'M3', 'M4', 'M5', 'M6', 'M7', 'M8', 'M9'] \n",
    "\n",
    "for col in matching_cols:\n",
    "    if col in df.columns:\n",
    "        print(f\"{col}: {df[col].nunique()} unique values\")\n",
    "        print(f'sample values: {df[col].dropna().unique()[:5]}')\n",
    "    else:\n",
    "        print(f\"{col} not in dataframe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8fce4f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#missing value in matching cols\n",
    "print(\"\\nMissing values in Matching Features:\")\n",
    "print(df[matching_cols].isnull().sum())\n",
    "print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcc72cd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# distribution of M1 to M9\n",
    "plt.figure(figsize=(30, 20))\n",
    "for i, col in enumerate(matching_cols):\n",
    "    if col in df.columns:\n",
    "        plt.subplot(3, 3, i+1)\n",
    "        sns.countplot(data=df, x=col, order=df[col].value_counts().index, palette=\"Set3\" , hue='isFraud')\n",
    "        plt.title(f'Distribution of {col}')\n",
    "        plt.xlabel(col)\n",
    "        plt.ylabel('Count')\n",
    "        plt.xticks(rotation=45)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1733c0be",
   "metadata": {},
   "source": [
    "## V columns "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ed861a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[[col for col in df.columns if col.startswith('V')]].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9255af18",
   "metadata": {},
   "outputs": [],
   "source": [
    "v_cols = [col for col in df.columns if col.startswith('V')]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0955b331",
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in v_cols:\n",
    "    if col in df.columns:\n",
    "        print(f\"{col}: {df[col].nunique()} unique values\")\n",
    "        print(f'sample values: {df[col].dropna().unique()[:5]}')\n",
    "    else:\n",
    "        print(f\"{col} not in dataframe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bf849bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "v_cols = [col for col in df.columns if col.startswith('V')] \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b145eeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df[[col for col in df.columns if col.startswith('V')]].describe()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb5d746e",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 12))\n",
    "sns.heatmap(df[ [f'V{i}' for i in range(1, 337 + 1)] + ['isFraud']].corr(), annot=False, cmap='coolwarm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d124aa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_iv(data: pd.DataFrame, feature: str, target: str):\n",
    "    crosstab = pd.crosstab(data[feature], data[target], normalize=False)\n",
    "    crosstab.columns = ['Good', 'Bad']\n",
    "    crosstab['Total'] = crosstab['Good'] + crosstab['Bad']\n",
    "    crosstab['Good%'] = crosstab['Good'] / crosstab['Good'].sum()\n",
    "    crosstab['Bad%'] = crosstab['Bad'] / crosstab['Bad'].sum()\n",
    "    crosstab = crosstab[(crosstab['Good%'] > 0) & (crosstab['Bad%'] > 0)]\n",
    "    crosstab['WOE'] = np.log(crosstab['Good%'] / crosstab['Bad%'])\n",
    "    crosstab['IV'] = (crosstab['Good%'] - crosstab['Bad%']) * crosstab['WOE']\n",
    "    return crosstab['IV'].sum()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0daebf1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_iv(data: pd.DataFrame, feature: str, target: str):\n",
    "    crosstab = pd.crosstab(data[feature], data[target], normalize=False)\n",
    "    crosstab.columns = ['Good', 'Bad']\n",
    "    crosstab['Total'] = crosstab['Good'] + crosstab['Bad']\n",
    "    crosstab['Good%'] = crosstab['Good'] / crosstab['Good'].sum()\n",
    "    crosstab['Bad%'] = crosstab['Bad'] / crosstab['Bad'].sum()\n",
    "    crosstab = crosstab[(crosstab['Good%'] > 0) & (crosstab['Bad%'] > 0)]\n",
    "    crosstab['WOE'] = np.log(crosstab['Good%'] / crosstab['Bad%'])\n",
    "    crosstab['IV'] = (crosstab['Good%'] - crosstab['Bad%']) * crosstab['WOE']\n",
    "    return crosstab['IV'].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d204a13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# top 10 features by IV\n",
    "iv_values = {}\n",
    "for col in df.columns:\n",
    "    if col != 'isFraud':\n",
    "        try:\n",
    "            iv = calculate_iv(df, col, 'isFraud')\n",
    "            iv_values[col] = iv\n",
    "        except Exception as e:\n",
    "            print(f\"Could not calculate IV for {col}: {e}\")\n",
    "\n",
    "\n",
    "# strong predictive power if IV > 0.3 and < 0.5\n",
    "iv_series = pd.Series(iv_values).sort_values(ascending=False)\n",
    "strong_predictive = iv_series[(iv_series > 0.3) & (iv_series < 0.5)]\n",
    "strong_predictive.head(10)\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94c45f2b",
   "metadata": {},
   "source": [
    "There top 10 feature with strong predictive power -> reduce dimension  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "800bd6b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot top 10 features by IV\n",
    "plt.figure(figsize=(10,6))\n",
    "sns.barplot(x=strong_predictive.head(10).values, y=strong_predictive.head(10).index, palette=\"viridis\")\n",
    "plt.title(\"Top 10 Features by Information Value (IV)\")\n",
    "plt.xlabel(\"Information Value (IV)\")\n",
    "plt.ylabel(\"Feature\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "133656e3",
   "metadata": {},
   "source": [
    "## Feature Identity "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab80c3c5",
   "metadata": {},
   "source": [
    "### Device type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ea08ceb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print sample value for device type and device info\n",
    "print(f\"DeviceType: {df['DeviceType'].nunique()} unique values\")\n",
    "print(f'sample values: {df[\"DeviceType\"].dropna().unique()[:5]}')\n",
    "print(f\"DeviceInfo: {df['DeviceInfo'].nunique()} unique values\")\n",
    "print(f'sample values: {df[\"DeviceInfo\"].dropna().unique()[:5]}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caba1b1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot stack bar of device type by isFraud\n",
    "device_type_stats = df.groupby(['DeviceType', 'isFraud']).size().unstack(fill_value=0)\n",
    "device_type_stats.columns = ['Legit', 'Fraud']\n",
    "ax = device_type_stats.plot(kind='bar', stacked=True, figsize=(8,6), color=['steelblue','tomato'])\n",
    "for container in ax.containers:\n",
    "    ax.bar_label(container, label_type='center', color='white', fontsize=9, fontweight='bold')\n",
    "plt.title(\"Transaction Distribution by Device Type\")\n",
    "plt.xlabel(\"Device Type\")\n",
    "plt.ylabel(\"Number of Transactions\")\n",
    "plt.legend([\"Legit\",\"Fraud\"])\n",
    "plt.xticks(rotation=0)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f112905b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample value of device info\n",
    "print(f\"DeviceInfo: {df['DeviceInfo'].nunique()} unique values\")\n",
    "print(f'sample values: {df[\"DeviceInfo\"].dropna().unique()[:50]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe0de6bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "pd.Series(df['id_31'].unique()).sample(5, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "623e97f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series(df['id_30'].unique()).sample(5, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f30c136c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.12.6)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
